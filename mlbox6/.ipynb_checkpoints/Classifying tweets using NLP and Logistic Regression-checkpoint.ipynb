{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick word before we beging:\n",
    "\n",
    "This notebook is adapted from Emmannual Ameisen's very useful article: https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\n",
    "\n",
    "Checkout his github for a longer exposition (and/or as a resource for wherever you/we might get stuck!):\n",
    "https://github.com/hundredblocks/concrete_NLP_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Table of Contents:<a name=\"TOC\"></a></h2>\n",
    "\n",
    "- [0. Problem Statement](#Objective)\n",
    "- [1. Data Preprocessing](#Data)\n",
    "- [2. NLP: Words to Vectors!](#NLP)\n",
    "- [3.Classification using LR](#LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>0. Problem Statement <a name=\"Objective\"></a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Dataset: Disasters on social media\n",
    "Contributors looked at over 10,000 tweets retrieved with a variety of searches like “ablaze”, “quarantine”, and “pandemonium”, then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous). Thank you [Crowdflower](https://www.crowdflower.com/data-for-everyone/).\n",
    "\n",
    "### Why it matters\n",
    "We will try to correctly predict tweets that are about disasters. This is a very relevant problem, because:\n",
    "- It is actionable to anybody trying to get signal from noise (such as police departments in this case)\n",
    "- It is tricky because relying on keywords is harder than in most cases like spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>1. Data Pre-processing <a name=\"Data\"></a></h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install nltk beforehand; if not uncomment and run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanitizing input\n",
    "Let's make sure our tweets only have characters we want. We remove '#' characters but keep the words after the '#' sign because they might be relevant (eg: #disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = codecs.open(\"socialmedia_relevant_cols.csv\", \"r\",encoding='utf-8', errors='replace')\n",
    "output_file = open(\"socialmedia_relevant_cols_clean.csv\", \"w\")\n",
    "\n",
    "def sanitize_characters(raw, clean):    \n",
    "    for line in input_file:\n",
    "        out = line\n",
    "        output_file.write(line)\n",
    "sanitize_characters(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  class_label\n",
       "0                 Just happened a terrible car crash   Relevant            1\n",
       "1  Our Deeds are the Reason of this #earthquake M...   Relevant            1\n",
       "2  Heard about #earthquake is different cities, s...   Relevant            1\n",
       "3  there is a forest fire at spot pond, geese are...   Relevant            1\n",
       "4             Forest fire near La Ronge Sask. Canada   Relevant            1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = pd.read_csv(\"socialmedia_relevant_cols_clean.csv\")\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this  earthquake m...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heard about  earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  class_label\n",
       "0                 just happened a terrible car crash   Relevant            1\n",
       "1  our deeds are the reason of this  earthquake m...   Relevant            1\n",
       "2  heard about  earthquake is different cities, s...   Relevant            1\n",
       "3  there is a forest fire at spot pond, geese are...   Relevant            1\n",
       "4             forest fire near la ronge sask  canada   Relevant            1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "questions = standardize_text(questions, \"text\")\n",
    "\n",
    "questions.to_csv(\"clean_data.csv\")\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this  earthquake m...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heard about  earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  class_label\n",
       "0                 just happened a terrible car crash   Relevant            1\n",
       "1  our deeds are the reason of this  earthquake m...   Relevant            1\n",
       "2  heard about  earthquake is different cities, s...   Relevant            1\n",
       "3  there is a forest fire at spot pond, geese are...   Relevant            1\n",
       "4             forest fire near la ronge sask  canada   Relevant            1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "questions = standardize_text(questions, \"text\")\n",
    "\n",
    "questions.to_csv(\"clean_data.csv\")\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6186</td>\n",
       "      <td>6186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4657</td>\n",
       "      <td>4657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             text  choose_one\n",
       "class_label                  \n",
       "0            6186        6186\n",
       "1            4657        4657\n",
       "2              16          16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_questions = pd.read_csv(\"clean_data.csv\",index_col=0)\n",
    "clean_questions.groupby(\"class_label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>class_label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, happened, a, terrible, car, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our deeds are the reason of this  earthquake m...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heard about  earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[heard, about, earthquake, is, different, citi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, is, a, forest, fire, at, spot, pond, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  class_label  \\\n",
       "0                 just happened a terrible car crash   Relevant            1   \n",
       "1  our deeds are the reason of this  earthquake m...   Relevant            1   \n",
       "2  heard about  earthquake is different cities, s...   Relevant            1   \n",
       "3  there is a forest fire at spot pond, geese are...   Relevant            1   \n",
       "4             forest fire near la ronge sask  canada   Relevant            1   \n",
       "\n",
       "                                              tokens  \n",
       "0          [just, happened, a, terrible, car, crash]  \n",
       "1  [our, deeds, are, the, reason, of, this, earth...  \n",
       "2  [heard, about, earthquake, is, different, citi...  \n",
       "3  [there, is, a, forest, fire, at, spot, pond, g...  \n",
       "4      [forest, fire, near, la, ronge, sask, canada]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "clean_questions[\"tokens\"] = clean_questions[\"text\"].apply(tokenizer.tokenize)\n",
    "clean_questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the following libraries too - if you don't have them installed, it's the same drill: un-comment and install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow\n",
    "#pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154484 words total, with a vocabulary size of 18095\n",
      "Max sentence length is 34\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "all_words = [word for tokens in clean_questions[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in clean_questions[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>2. NLP: Converting the text to vectors! <a name=\"NLP\"></a></h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of Words (Count)\n",
    "The simplest approach we can start with is to use a bag of words model, and apply a logistic regression on top. A bag of words just associates an index to each word in our vocabulary, and embeds each sentence as a list of 0s, with a 1 at each index corresponding to a word present in the sentence.\n",
    "\n",
    "We're will use sklearn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer) for this.\n",
    "\n",
    "### 2. TFIDF Bag of Words\n",
    "A slightly more subtle approach would be to use TFIDF. On top of our bag of words model, we use a TF-IDF (Term Frequency, Inverse Document Frequency) which means weighing words by how frequent they are in our dataset, discounting words that are too frequent, as they just add to the noise.\n",
    "\n",
    "We're going to use sklearn's [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "# def tfidf(data):\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#     train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "#     return train, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus = clean_questions[\"text\"].tolist()\n",
    "list_labels = clean_questions[\"class_label\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n",
    "                                                                                random_state=40)\n",
    "\n",
    "X_train_counts, count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>3. Classification using LR <a name=\"LR\"></a></h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Some useful Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.winter):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=30)\n",
    "    plt.xlabel('Predicted label', fontsize=30)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the most important words: Using the LR Co-efficients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "#print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))\n",
    "#importance = get_most_important_features(count_vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('Irrelevant', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Disaster', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_scores = [a[0] for a in importance[1]['tops']]\n",
    "# top_words = [a[1] for a in importance[1]['tops']]\n",
    "# bottom_scores = [a[0] for a in importance[1]['bottom']]\n",
    "# bottom_words = [a[1] for a in importance[1]['bottom']]\n",
    "\n",
    "# plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
